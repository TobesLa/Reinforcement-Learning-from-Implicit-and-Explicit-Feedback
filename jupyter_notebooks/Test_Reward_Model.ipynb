{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory (where the notebook is located)\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Navigate to the parent directory\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from Reward_Models.Reward_Model_Trajectory import RewardModelTrajectory\n",
    "from Reward_Models.Reward_Model_Stepwise import RewardModelStepwise\n",
    "from Agents.Feedback_Agent_Stepwise import FeedbackAgentStepwise\n",
    "import gymnasium as gym\n",
    "from Feedback.Feedback_FileHandler import FeedbackFileHandler\n",
    "import torch as torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "def print_q_vals_and_reward(reward_model):\n",
    "    for s, a in feedback:\n",
    "        for a_prime in range(4):\n",
    "            print()\n",
    "            if a_prime == a:\n",
    "                print('should be optimal!')\n",
    "            #print('q-value for state action pair: ', (s, a_prime), 'q: ', feedback_q_table[s][a_prime])\n",
    "            print('reward for state action pair ', (s, a_prime), 'r: ', reward_model.get_reward(s, a_prime))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stepwise RM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(36, 0): 1, (24, 1): 1, (25, 1): 1, (26, 1): 1, (27, 1): 1, (28, 1): 1, (29, 1): 1, (30, 1): 1, (31, 1): 1, (32, 1): 1, (33, 1): 1, (34, 1): 1, (35, 2): 1}\n",
      "tensor([ 3.1262, -3.1684, -3.1684, -3.1684], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "env_name = 'CliffWalking-v0'\n",
    "#env_name = \"FrozenLake-v1\"\n",
    "learning_rate = 0.1\n",
    "discount = 0.99\n",
    "epsilon = 1\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "exploration_decay_rate = 0.01\n",
    "num_q_updates = 10\n",
    "feedback_var =  'SRF'\n",
    "noise = 0\n",
    "delay = 0.1\n",
    "batches = 1\n",
    "start_state = 36 # Cliff_walking\n",
    "#start_state = 0 # Frozenlake\n",
    "optimal_val = -13 # Cliff_waling\n",
    "#optimal_val = 1 # Frozenlake\n",
    "\n",
    "\n",
    "file_path = os.path.join(parent_dir, rf\"Feedback\\{env_name}_feedback.txt\")\n",
    "\n",
    "#file_path = rf\"C:\\Users\\PC\\Desktop\\Uni\\Bachelorarbeit\\Coding\\repo\\bt_lascsak\\Feedback\\{env_name}_feedback.txt\"\n",
    "feedback_filehandler = FeedbackFileHandler(file_path)\n",
    "feedback = feedback_filehandler.load_dict_from_file()\n",
    "\n",
    "agent = FeedbackAgentStepwise(env_name, feedback, learning_rate, discount, epsilon,\n",
    "                              exploration_decay_rate, max_steps_per_episode,\n",
    "                              num_episodes, num_q_updates, noise, delay, batches, feedback_var)\n",
    "print(feedback)\n",
    "env = gym.make(env_name)\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "rm = RewardModelStepwise(num_states, num_actions, batches, learning_rate)\n",
    "#rm.to('cuda')\n",
    "states_optimal = [start_state]\n",
    "states_bad = [start_state, start_state, start_state]\n",
    "actions_bad = [1, 2, 3]\n",
    "actions_optimal = [0]\n",
    "test_trajectory_good = [(start_state, 0)]\n",
    "test_trajectory_bad = [(start_state, 1), (start_state, 2), (start_state, 3)]\n",
    "test_trajectory_optimal =[[*feedback]]\n",
    "for i in range(30):\n",
    "    rm.update_reward_model(states_bad, actions_bad, [0.0])\n",
    "    rm.update_reward_model(states_optimal, actions_optimal, [1.0])\n",
    "\n",
    "#print(rm.get_reward(start_state,0), ' ', rm.get_reward(start_state,1), ' ',rm.get_reward(start_state,2), ' ', rm.get_reward(start_state,3), ' ',)\n",
    "print(rm.rewards[start_state][:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trajectory RM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(36, 0): 1, (24, 1): 1, (25, 1): 1, (26, 1): 1, (27, 1): 1, (28, 1): 1, (29, 1): 1, (30, 1): 1, (31, 1): 1, (32, 1): 1, (33, 1): 1, (34, 1): 1, (35, 2): 1}\n",
      "[[(36, 0)]]\n",
      "[1.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = os.path.join(parent_dir, rf\"Feedback\\{env_name}_feedback.txt\")\n",
    "\n",
    "#file_path = rf\"C:\\Users\\PC\\Desktop\\Uni\\Bachelorarbeit\\Coding\\repo\\bt_lascsak\\Feedback\\{env_name}_feedback.txt\"\n",
    "feedback_filehandler = FeedbackFileHandler(file_path)\n",
    "feedback = feedback_filehandler.load_dict_from_file()\n",
    "print(feedback)\n",
    "env = gym.make(env_name)\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "rm = RewardModelTrajectory(num_states, num_actions, batches, learning_rate, noise)\n",
    "#rm.to('cuda')\n",
    "test_trajectory_good = [[(start_state, 0, 1)]]\n",
    "feedback_good = [agent._get_feedback_SRF(traj) for traj in test_trajectory_good]\n",
    "test_trajectory_good = [[(state, action) for traj in test_trajectory_good for state, action, _ in traj]]\n",
    "print(test_trajectory_good)\n",
    "print(feedback_good)\n",
    "test_trajectory_bad = [[(start_state, 1,2), (start_state, 2,2), (start_state, 3,2)]]\n",
    "feedback_bad = [agent._get_feedback_SRF(traj) for traj in test_trajectory_bad]\n",
    "test_trajectory_bad = [[(state, action) for traj in test_trajectory_bad for state, action, _ in traj]]\n",
    "\n",
    "trajectory_decent = list_of_tuples = [[\n",
    "    (36, 3,1),\n",
    "    (24, 2,1),\n",
    "    (25, 1,1),\n",
    "    (26, 3,1),\n",
    "    (27, 1,1),\n",
    "    (28, 0,1),\n",
    "    (29, 0,1),\n",
    "    (30, 0,1),\n",
    "    (31, 1,1),\n",
    "    (32, 3,1),\n",
    "    (33, 2,1),\n",
    "    (34, 1,1),\n",
    "    (35, 0,1),\n",
    "    (1,0,1),\n",
    "    (4,3,1),\n",
    "    (25,3,1),\n",
    "    (27,0,1),\n",
    "    (12,3,1),\n",
    "    (23,2,1),\n",
    "    (3,2,1)\n",
    "]]\n",
    "feedback_decent =[agent._get_feedback_SRF(traj) for traj in trajectory_decent]\n",
    "trajectory_decent = [[(state, action) for traj in trajectory_decent for state, action, _ in traj]]\n",
    "\n",
    "\n",
    "test_trajectory_optimal =[[*feedback]]\n",
    "test_trajectory_optimal_prime = [[(state, action, 1) for  traj in test_trajectory_optimal for state, action in traj]]\n",
    "feedback_opt = [agent._get_feedback_SRF(traj) for traj in test_trajectory_optimal_prime]\n",
    "all_trajs = [test_trajectory_good[0], test_trajectory_bad[0],trajectory_decent[0],test_trajectory_optimal[0]]\n",
    "all_feedbacks = [feedback_good[0], feedback_bad[0], feedback_decent[0],feedback_opt[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (36, 0) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (36, 1) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (36, 2) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (36, 3) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (24, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (24, 1) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (24, 2) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (24, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (25, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (25, 1) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (25, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (25, 3) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (26, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (26, 1) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (26, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (26, 3) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (27, 0) r:  -0.5791217684745789\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (27, 1) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (27, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (27, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (28, 0) r:  -0.5791217684745789\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (28, 1) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (28, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (28, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (29, 0) r:  -0.5791217684745789\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (29, 1) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (29, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (29, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (30, 0) r:  -0.5791217684745789\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (30, 1) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (30, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (30, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (31, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (31, 1) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (31, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (31, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (32, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (32, 1) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (32, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (32, 3) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (33, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (33, 1) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (33, 2) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (33, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (34, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (34, 1) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (34, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (34, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (35, 0) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (35, 1) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (35, 2) r:  -0.5791217684745789\n",
      "\n",
      "reward for state action pair  (35, 3) r:  0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    #rm.update_reward_model(test_trajectory_bad, feedback_bad)\n",
    "    #rm.update_reward_model(test_trajectory_bad, feedback_bad)\n",
    "    #rm.update_reward_model(test_trajectory_good, feedback_good)\n",
    "    #rm.update_reward_model(trajectory_decent, feedback_decent)\n",
    "    #rm.update_reward_model(test_trajectory_optimal, feedback_opt)\n",
    "    rm.update_reward_model(all_trajs,all_feedbacks)\n",
    "\n",
    "\n",
    "print_q_vals_and_reward(rm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (36, 0) r:  -0.6020482182502747\n",
      "\n",
      "reward for state action pair  (36, 1) r:  -1.0077500343322754\n",
      "\n",
      "reward for state action pair  (36, 2) r:  -1.0077500343322754\n",
      "\n",
      "reward for state action pair  (36, 3) r:  -1.007750153541565\n",
      "\n",
      "reward for state action pair  (24, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (24, 1) r:  -0.3326643705368042\n",
      "\n",
      "reward for state action pair  (24, 2) r:  -1.0077500343322754\n",
      "\n",
      "reward for state action pair  (24, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (25, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (25, 1) r:  -0.6020482182502747\n",
      "\n",
      "reward for state action pair  (25, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (25, 3) r:  -1.0077500343322754\n",
      "\n",
      "reward for state action pair  (26, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (26, 1) r:  -0.3326643705368042\n",
      "\n",
      "reward for state action pair  (26, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (26, 3) r:  -1.0077500343322754\n",
      "\n",
      "reward for state action pair  (27, 0) r:  -1.0077500343322754\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (27, 1) r:  -0.6020482182502747\n",
      "\n",
      "reward for state action pair  (27, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (27, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (28, 0) r:  -1.0077500343322754\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (28, 1) r:  -0.3326643705368042\n",
      "\n",
      "reward for state action pair  (28, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (28, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (29, 0) r:  -1.0077500343322754\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (29, 1) r:  -0.3326643705368042\n",
      "\n",
      "reward for state action pair  (29, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (29, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (30, 0) r:  -1.0077500343322754\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (30, 1) r:  -0.3326643705368042\n",
      "\n",
      "reward for state action pair  (30, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (30, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (31, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (31, 1) r:  -0.6020482182502747\n",
      "\n",
      "reward for state action pair  (31, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (31, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (32, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (32, 1) r:  -0.3326643705368042\n",
      "\n",
      "reward for state action pair  (32, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (32, 3) r:  -1.0077500343322754\n",
      "\n",
      "reward for state action pair  (33, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (33, 1) r:  -0.3326643705368042\n",
      "\n",
      "reward for state action pair  (33, 2) r:  -1.0077500343322754\n",
      "\n",
      "reward for state action pair  (33, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (34, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (34, 1) r:  -0.6020482182502747\n",
      "\n",
      "reward for state action pair  (34, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (34, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (35, 0) r:  -1.0077500343322754\n",
      "\n",
      "reward for state action pair  (35, 1) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (35, 2) r:  -0.3326643705368042\n",
      "\n",
      "reward for state action pair  (35, 3) r:  0.0\n"
     ]
    }
   ],
   "source": [
    "all_trajs.remove(test_trajectory_optimal[0])\n",
    "all_feedbacks.remove(feedback_opt[0])\n",
    "\n",
    "for i in range(100):\n",
    "    #rm.update_reward_model(test_trajectory_bad, feedback_bad)\n",
    "    #rm.update_reward_model(test_trajectory_bad, feedback_bad)\n",
    "    #rm.update_reward_model(test_trajectory_good, feedback_good)\n",
    "    #rm.update_reward_model(trajectory_decent, feedback_decent)\n",
    "    #rm.update_reward_model(test_trajectory_optimal, feedback_opt)\n",
    "    rm.update_reward_model(all_trajs,all_feedbacks)\n",
    "\n",
    "\n",
    "\n",
    "print_q_vals_and_reward(rm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# from a normal training run through\n",
    "random_traj1 = [(36, 0, 24), (24, 1, 25), (25, 1, 26), (26, 0, 14), (14, 0, 2), (2, 2, 14), (14, 0, 2), (2, 3, 1), (1, 1, 2), (2, 1, 3), (3, 3, 2), (2, 2, 14), (14, 3, 13), (13, 2, 25), (25, 3, 24), (24, 1, 25), (25, 1, 26), (26, 2, 36), (36, 2, 36), (36, 0, 24), (24, 0, 12), (12, 0, 0), (0, 0, 0), (0, 1, 1), (1, 2, 13), (13, 1, 14), (14, 3, 13), (13, 0, 1), (1, 1, 2), (2, 3, 1), (1, 3, 0), (0, 0, 0), (0, 2, 12), (12, 0, 0), (0, 0, 0), (0, 1, 1), (1, 3, 0), (0, 1, 1), (1, 3, 0), (0, 3, 0), (0, 1, 1), (1, 0, 1), (1, 0, 1), (1, 2, 13), (13, 2, 25), (25, 2, 36), (36, 0, 24), (24, 3, 24), (24, 3, 24), (24, 1, 25), (25, 3, 24), (24, 3, 24), (24, 1, 25), (25, 2, 36), (36, 0, 24), (24, 1, 25), (25, 2, 36), (36, 0, 24), (24, 1, 25), (25, 3, 24), (24, 1, 25), (25, 1, 26), (26, 1, 27), (27, 0, 15), (15, 3, 14), (14, 1, 15), (15, 1, 16), (16, 3, 15), (15, 0, 3), (3, 2, 15), (15, 1, 16), (16, 3, 15), (15, 1, 16), (16, 3, 15), (15, 3, 14), (14, 2, 26), (26, 0, 14), (14, 1, 15), (15, 2, 27), (27, 0, 15), (15, 1, 16), (16, 1, 17), (17, 3, 16), (16, 3, 15), (15, 1, 16), (16, 1, 17), (17, 1, 18), (18, 3, 17), (17, 3, 16), (16, 0, 4), (4, 2, 16), (16, 0, 4), (4, 2, 16), (16, 3, 15), (15, 1, 16), (16, 3, 15), (15, 0, 3), (3, 1, 4), (4, 0, 4), (4, 0, 4)]\n",
    "\n",
    "random_traj2 =[(36, 2, 36), (36, 0, 24), (24, 1, 25), (25, 2, 36), (36, 1, 36), (36, 3, 36), (36, 0, 24), (24, 2, 36), (36, 0, 24), (24, 2, 36), (36, 0, 24), (24, 0, 12), (12, 1, 13), (13, 0, 1), (1, 2, 13), (13, 3, 12), (12, 1, 13), (13, 1, 14), (14, 0, 2), (2, 3, 1), (1, 1, 2), (2, 2, 14), (14, 1, 15), (15, 0, 3), (3, 2, 15), (15, 0, 3), (3, 3, 2), (2, 1, 3), (3, 2, 15), (15, 1, 16), (16, 3, 15), (15, 3, 14), (14, 0, 2), (2, 3, 1), (1, 3, 0), (0, 2, 12), (12, 3, 12), (12, 0, 0), (0, 0, 0), (0, 0, 0), (0, 1, 1), (1, 3, 0), (0, 3, 0), (0, 1, 1), (1, 3, 0), (0, 0, 0), (0, 0, 0), (0, 3, 0), (0, 1, 1), (1, 1, 2), (2, 3, 1), (1, 1, 2), (2, 0, 2), (2, 0, 2), (2, 2, 14), (14, 2, 26), (26, 2, 36), (36, 3, 36), (36, 2, 36), (36, 0, 24), (24, 3, 24), (24, 0, 12), (12, 0, 0), (0, 0, 0), (0, 3, 0), (0, 3, 0), (0, 1, 1), (1, 0, 1), (1, 1, 2), (2, 2, 14), (14, 2, 26), (26, 2, 36), (36, 1, 36), (36, 0, 24), (24, 3, 24), (24, 0, 12), (12, 2, 24), (24, 2, 36), (36, 0, 24), (24, 3, 24), (24, 0, 12), (12, 1, 13), (13, 0, 1), (1, 2, 13), (13, 0, 1), (1, 1, 2), (2, 2, 14), (14, 1, 15), (15, 0, 3), (3, 1, 4), (4, 1, 5), (5, 1, 6), (6, 2, 18), (18, 1, 19), (19, 3, 18), (18, 0, 6), (6, 0, 6), (6, 2, 18), (18, 1, 19), (19, 3, 18)]\n",
    "\n",
    "\n",
    "rm = RewardModelTrajectory(num_states, num_actions, 3, learning_rate, noise)\n",
    "random_feedback1 = agent._get_feedback_SRF(random_traj1)\n",
    "random_feedback2 = agent._get_feedback_SRF(random_traj2)\n",
    "random_traj1 = [(state,action) for state,action,_ in random_traj1 ]\n",
    "random_traj2 = [(state,action) for state,action,_ in random_traj2 ]\n",
    "all_trajs = [random_traj1, test_trajectory_optimal[0], random_traj2]\n",
    "all_feedbacks = [random_feedback1, feedback_opt[0],random_feedback2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (36, 0) r:  -0.875147819519043\n",
      "\n",
      "reward for state action pair  (36, 1) r:  -0.8751475811004639\n",
      "\n",
      "reward for state action pair  (36, 2) r:  -0.875147819519043\n",
      "\n",
      "reward for state action pair  (36, 3) r:  -0.8751475811004639\n",
      "\n",
      "reward for state action pair  (24, 0) r:  -0.8751478791236877\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (24, 1) r:  -0.875147819519043\n",
      "\n",
      "reward for state action pair  (24, 2) r:  -0.875147819519043\n",
      "\n",
      "reward for state action pair  (24, 3) r:  -0.875147819519043\n",
      "\n",
      "reward for state action pair  (25, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (25, 1) r:  -0.8751478791236877\n",
      "\n",
      "reward for state action pair  (25, 2) r:  -0.8751478791236877\n",
      "\n",
      "reward for state action pair  (25, 3) r:  -0.875147819519043\n",
      "\n",
      "reward for state action pair  (26, 0) r:  -0.8751475811004639\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (26, 1) r:  -0.8751475811004639\n",
      "\n",
      "reward for state action pair  (26, 2) r:  -0.875147819519043\n",
      "\n",
      "reward for state action pair  (26, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (27, 0) r:  -0.8751475811004639\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (27, 1) r:  -0.8751476407051086\n",
      "\n",
      "reward for state action pair  (27, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (27, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (28, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (28, 1) r:  -0.8751476407051086\n",
      "\n",
      "reward for state action pair  (28, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (28, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (29, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (29, 1) r:  -0.8751476407051086\n",
      "\n",
      "reward for state action pair  (29, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (29, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (30, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (30, 1) r:  -0.8751476407051086\n",
      "\n",
      "reward for state action pair  (30, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (30, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (31, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (31, 1) r:  -0.8751476407051086\n",
      "\n",
      "reward for state action pair  (31, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (31, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (32, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (32, 1) r:  -0.8751476407051086\n",
      "\n",
      "reward for state action pair  (32, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (32, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (33, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (33, 1) r:  -0.8751476407051086\n",
      "\n",
      "reward for state action pair  (33, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (33, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (34, 0) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (34, 1) r:  -0.8751476407051086\n",
      "\n",
      "reward for state action pair  (34, 2) r:  0.0\n",
      "\n",
      "reward for state action pair  (34, 3) r:  0.0\n",
      "\n",
      "reward for state action pair  (35, 0) r:  0.0\n",
      "\n",
      "reward for state action pair  (35, 1) r:  0.0\n",
      "\n",
      "should be optimal!\n",
      "reward for state action pair  (35, 2) r:  -0.8751476407051086\n",
      "\n",
      "reward for state action pair  (35, 3) r:  0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    #rm.update_reward_model(test_trajectory_bad, feedback_bad)\n",
    "    #rm.update_reward_model(test_trajectory_bad, feedback_bad)\n",
    "    #rm.update_reward_model(test_trajectory_good, feedback_good)\n",
    "    #rm.update_reward_model(trajectory_decent, feedback_decent)\n",
    "    #rm.update_reward_model(test_trajectory_optimal, feedback_opt)\n",
    "    rm.update_reward_model(all_trajs,all_feedbacks)\n",
    "\n",
    "\n",
    "print_q_vals_and_reward(rm)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
